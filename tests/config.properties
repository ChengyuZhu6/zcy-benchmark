inference_address=http://0.0.0.0:8085
management_address=http://0.0.0.0:8086
metrics_address=http://0.0.0.0:8082
grpc_inference_port=7070
grpc_management_port=7071
max_request_size=1073741824
max_response_size=1073741824
install_py_dep_per_model=false
NUM_WORKERS=24
number_of_gpu=0
number_of_netty_threads=24
job_queue_size=1000
model_store=/mnt/models/
#cpu_launcher_enable=true
#cpu_launcher_args=--use_logical_core
cpu_launcher_args="--use_logical_core --disable-numactl --disable-taskset"
model_snapshot={"name":"startup.cfg","modelCount":1,"models":{"llama2-7b-ipex":{"1.0":{"defaultVersion":true,"marName":"llama2-7b-ipex","minWorkers":1,"maxWorkers":5,"batchSize":1,"maxBatchDelay":50000,"responseTimeout":50000}}}}
